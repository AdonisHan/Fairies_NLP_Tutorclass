{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import numpy as np\n",
    "import mglearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# puple machine - 07.LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 영화 리뷰 데이터셋\n",
    "- 비지도 학습 모델에서 분석결과가 왜곡되는걸 방지하기 위해 자주나타나는 단어 등 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of text_train: <class 'list'>\n",
      "length of text_train: 75000\n",
      "text_train[6]:\n",
      "b'Gloomy Sunday - Ein Lied von Liebe und Tod directed by Rolf Sch\\xc3\\xbcbel in 1999 is a romantic, absorbing, beautiful, and heartbreaking movie. It started like Jules and Jim; it ended as one of Agatha Christie\\'s books, and in between it said something about love, friendship, devotion, jealousy, war, Holocaust, dignity, and betrayal, and it did better than The Black Book which is much more popular. It is not perfect, and it made me, a cynic, wonder in the end on the complexity of the relationships and sensational revelations, and who is who to whom but the movie simply overwhelmed me. Perfect or not, it is unforgettable. All four actors as the parts of the tragic not even a triangle but a rectangle were terrific. I do believe that three men could fell deeply for one girl as beautiful and dignified as Ilona in a star-making performance by young Hungarian actress Erica Marozs\\xc3\\xa1n and who would not? The titular song is haunting, sad, and beautiful, and no doubt deserves the movie been made about it and its effect on the countless listeners. I love the movie and I am surprised that it is so little known in this country. It is a gem.<br /><br />The fact that it is based on a story of the song that had played such important role in the lives of all characters made me do some research, and the real story behind the song of Love and Death seems as fascinating as the fictional one. The song was composed in 1930s by Rezs\\xc3\\xb6 Seress and was believed to have caused many suicides in Hungary and all over Europe as the world was moving toward the most devastating War of the last century. Rezs\\xc3\\xb6 Seress, a Jewish-Hungarian pianist and composer, was thrown to the Concentration Camp but survived, unlike his mother. In January, 1968, Seress committed suicide in Budapest by jumping out of a window. According to his obituary in the New York Times, \"Mr. Seres complained that the success of \"Gloomy Sunday\" actually increased his unhappiness, because he knew he would never be able to write a second hit.\" <br /><br />Many singers from all over the world have recorded their versions of the songs in different languages. Over 70 performers have covered the song since 1935, and some famous names include Billie Holiday, Paul Robeson, Pyotr Leschenko (in Russian, under title \"Mratschnoje Woskresenje\"), Bjork, Sarah McLachlan, and many more. The one that really got to me and made me shiver is by Diamanda Gal\\xc3\\xa1s, the Greek born American singer/pianist/performer with the voice of such tragic power that I still can\\'t get over her singing. Gal\\xc3\\xa1s has been described as \"capable of the most unnerving vocal terror\", and in her work she mostly concentrates on the topics of \"suffering, despair, condemnation, injustice and loss of dignity.\" When she sings the Song of Love and Death, her voice that could\\'ve belonged to the most tragic heroines of Ancient Greece leaves no hope and brings the horror and grief of love lost forever to the unbearable and incomparable heights.<br /><br />8.5/10'\n"
     ]
    }
   ],
   "source": [
    "reviews_train = load_files(\"C:/Users/Adonishan/basic/introduction_to_ml_with_python-master/data/aclImdb/train/\")\n",
    "# load_files returns a bunch, containing training texts and training labels\n",
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "print(\"type of text_train: {}\".format(type(text_train)))\n",
    "print(\"length of text_train: {}\".format(len(text_train)))\n",
    "print(\"text_train[6]:\\n{}\".format(text_train[6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 문서에서 나타나는 단어에 15% 나타나는 단어를 삭제\n",
    "- 가장 많이 등장하는 10,000개에 대한 Bag Of Word 모델 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(max_features=10000, max_df=.15)\n",
    "X = vect.fit_transform(text_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA function build\n",
    "\n",
    "- We build the model and transform the data in one step. (모델 생성과 변환을 한 번에)\n",
    "- Computing transform takes some time, and we can save time by doing both at once(변환 시간이 좀 걸리므로 시간을 절약하기 위해 동시처리) \n",
    "- n_topic = 10; 10개의 토픽으로 토픽 모델 학습할 것. 토픽의 수를 바꾸면 모든 토픽이 바뀐다.\n",
    "- batch 방법을 사용\n",
    "- max_iter :기본값 10; 모델 성능을 위해 max_iter 값 증가."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adonishan\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# LDA\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_topics=10, learning_method=\"batch\",\n",
    "                                max_iter=25, random_state=0)\n",
    "\n",
    "document_topics = lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lda.components_.shape: (10, 10000)\n"
     ]
    }
   ],
   "source": [
    "print(\"lda.components_.shape: {}\".format(lda.components_.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each topic (a row in the components_), sort the features (ascending).\n",
    "# Invert rows with [:, ::-1] to make sorting descending \n",
    "# 내림차순이 되도록 [:, ::-1]을 사용해 행의 정렬을 반대로 바꾼다.\n",
    "\n",
    "sorting = np.argsort(lda.components_, axis=1)[:, ::-1]\n",
    "# get the feature names from the vectorizer:\n",
    "feature_names = np.array(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mglearn 으로 10개 토픽을 출력.\n",
    "# topic per chunk \n",
    "'''\n",
    "mglearn/tool.py\n",
    "def print_topics(topics, feature_names, sorting, topics_per_chunk=6,\n",
    "                 n_words=20):\n",
    "    for i in range(0, len(topics), topics_per_chunk):\n",
    "        # for each chunk:\n",
    "        these_topics = topics[i: i + topics_per_chunk]\n",
    "        # maybe we have less than topics_per_chunk left\n",
    "        len_this_chunk = len(these_topics)\n",
    "        # print topic headers\n",
    "        print((\"topic {:<8}\" * len_this_chunk).format(*these_topics))\n",
    "        print((\"-------- {0:<5}\" * len_this_chunk).format(\"\"))\n",
    "        # print top n_words frequent words\n",
    "        for i in range(n_words):\n",
    "            try:\n",
    "                print((\"{:<14}\" * len_this_chunk).format(\n",
    "                    *feature_names[sorting[these_topics, i]]))\n",
    "            except:\n",
    "                pass\n",
    "        print(\"\\n\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0       topic 1       topic 2       topic 3       topic 4       \n",
      "--------      --------      --------      --------      --------      \n",
      "action        guy           war           show          didn          \n",
      "game          re            world         series        thought       \n",
      "effects       around        american      tv            actors        \n",
      "special       nothing       us            episode       nothing       \n",
      "fight         sex           our           years         am            \n",
      "fi            looks         documentary   old           10            \n",
      "sci           pretty        history       now           worst         \n",
      "10            look          real          kids          going         \n",
      "star          worst         black         dvd           got           \n",
      "alien         stupid        america       shows         actually      \n",
      "\n",
      "\n",
      "topic 5       topic 6       topic 7       topic 8       topic 9       \n",
      "--------      --------      --------      --------      --------      \n",
      "director      horror        funny         father        role          \n",
      "work          gore          comedy        wife          cast          \n",
      "between       killer        girl          woman         john          \n",
      "us            original      doesn         young         performance   \n",
      "real          dead          humor         family        actor         \n",
      "may           blood         school        mother        plays         \n",
      "without       scary         fun           son           played        \n",
      "both          quite         guy           police        star          \n",
      "audience      budget        laugh         gets          screen        \n",
      "yet           genre         jokes         home          james         \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print out the 10 topics:\n",
    "mglearn.tools.print_topics(topics=range(10), feature_names=feature_names,\n",
    "                           sorting=sorting, topics_per_chunk=5, n_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 결과를 보면 \n",
    "- 0:  가족물\n",
    "- 1: 역사와 전쟁 영화처럼 보이고 \n",
    "- 2: 코미디물\n",
    "- 3: TV 시리즈물\n",
    "- 4: 일반적인 단어\n",
    "- 5: 영화용어 (약함)\n",
    "- 6: 키드물\n",
    "- 7 : 영화용어 (약함)\n",
    "- 8 : 리뷰\n",
    "- 9 : 영화용어 (약함)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 100개의 토픽 LDA\n",
    "- 재밌는 것만 뽑기.\n",
    "- 예를들어 음악 관련 토픽 45를 가중치로 정렬한다.\n",
    "    -  이 토픽의 비중이 가장 크 문서 다섯 개를 출력한다.\n",
    "    -  첫 두 문장을 출력 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda100 = LatentDirichletAllocation(n_topics=100, learning_method=\"batch\",\n",
    "                                   max_iter=25, random_state=0)\n",
    "document_topics100 = lda100.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = np.array([7, 16, 24, 25, 28, 36, 37, 41, 45, 51, 53, 54, 63, 89, 97])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting = np.argsort(lda100.components_, axis=1)[:, ::-1]\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "mglearn.tools.print_topics(topics=topics, feature_names=feature_names,\n",
    "                           sorting=sorting, topics_per_chunk=5, n_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by weight of \"music\" topic 45\n",
    "music = np.argsort(document_topics100[:, 45])[::-1]\n",
    "# print the five documents where the topic is most important\n",
    "for i in music[:10]:\n",
    "    # show first two sentences\n",
    "    print(b\".\".join(text_train[i].split(b\".\")[:2]) + b\".\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 10))\n",
    "topic_names = [\"{:>2} \".format(i) + \" \".join(words)\n",
    "               for i, words in enumerate(feature_names[sorting[:, :2]])]\n",
    "# two column bar chart:\n",
    "for col in [0, 1]:\n",
    "    start = col * 50\n",
    "    end = (col + 1) * 50\n",
    "    ax[col].barh(np.arange(50), np.sum(document_topics100, axis=0)[start:end])\n",
    "    ax[col].set_yticks(np.arange(50))\n",
    "    ax[col].set_yticklabels(topic_names[start:end], ha=\"left\", va=\"top\")\n",
    "    ax[col].invert_yaxis()\n",
    "    ax[col].set_xlim(0, 2000)\n",
    "    yax = ax[col].get_yaxis()\n",
    "    yax.set_tick_params(pad=130)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 인사이트\n",
    "- 빈도가 많은 단어는 불용어에 가깝고\n",
    "- 어떠한 토픽은 부정적으로 보이고 칭찬멘트가 많은 단어들로 구성된 것들이 있는지 알아낼수 있다.\n",
    "- 특정 영화에 대한 의견이거나 평가 점수를 합리화 하거나 강조하기 위한 댓글이라는 사실이 재미있다.\n",
    "- LDA라는 토픽모델은 라벨이 없거나 있어도 상광넚이 텍스트 말뭉치를 해석하는데 도움을 준다.\n",
    "- LDA는 확률적 알고리즘이기때문에 random_state 를 바꾸면 결과가 바뀌니 주의하자.\n",
    "- 보수적으로 평가해야 하는 이유는 비지도학습이기 때문에 과정을 이해하는데 해당 하는 문서를 직접 보지 않고 판단하는건 좋지 않다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
