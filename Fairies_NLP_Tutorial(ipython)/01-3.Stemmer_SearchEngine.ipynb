{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import alpino\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.corpus import webtext\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "import nltk\n",
    "from nltk.collocations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat\n"
     ]
    }
   ],
   "source": [
    "# Stemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import RegexpStemmer\n",
    "stemmerregexp=RegexpStemmer('ing')\n",
    "stemmerlan=LancasterStemmer()\n",
    "stemmerporter = PorterStemmer()\n",
    "print(stemmerporter.stem('eatting'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mangent\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "# 잘 안되는데 불어...\n",
    "frenchstemmer=SnowballStemmer('french')\n",
    "print(frenchstemmer.stem('mangent'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work\n",
      "work\n"
     ]
    }
   ],
   "source": [
    "# Lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer_output=WordNetLemmatizer()\n",
    "print(lemmatizer_output.lemmatize('working',pos='v'))\n",
    "print(lemmatizer_output.lemmatize('works'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "# Sentence length\n",
    "import nltk\n",
    "corpus=u\"<s> hello how are you doing ? Hope you find the book interesting. </s>\".split()\n",
    "sentence=u\"<s>how are you doing</s>\".split()\n",
    "vocabulary=set(corpus)\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "cfd = nltk.ConditionalFreqDist(nltk.bigrams(corpus))\n",
    "print([cfd[a][b] for (a,b) in nltk.bigrams(sentence)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "print([cfd[a].N() for (a,b) in nltk.bigrams(sentence)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print([cfd[a].freq(b) for (a,b) in nltk.bigrams(sentence)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문맥 관점에서 자주 발생하고 중요하지 않은 단어 제거\n",
    "def eliminatestopwords(self,list):\n",
    "    return [ word for word in list if word not in self.stopwords]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트를 불용어와 토큰으로 분할하는 작업을 수행\n",
    "def tokenize(self, string):\n",
    "    Str = self.clean(str)\n",
    "    Words = str.split(\"\")\n",
    "    return [self.stemmer.stem(word,0,len(word)-1) for word in words]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 키워드를 벡터 차원에 매핑\n",
    "def obtainvectorkeywordindex(self, documentList):\n",
    "    # Text 를 문자열로 매핑\n",
    "    vocabstring = \"\".join(documentList)\n",
    "    vocablist = self.parser.tokenise(vocabstring)\n",
    "    \n",
    "    # 검색의 중요성이 없는 일반적인 단어 제거\n",
    "    vocablist = self.parser.eliminatestopwords(vocablist)\n",
    "    uniqueVocablist = util.removeDuplicates(vocablist)\n",
    "    \n",
    "    vectorIndex = {}\n",
    "    offset = 0\n",
    "    # 이 토큰을 설명하는 데 사용되는 차원과의 매핑을 수행하는 키워드에 포지션을 연결.\n",
    "    \n",
    "    for word in uniqueVocablist:\n",
    "        vectorIndex[word]=offset\n",
    "        offset += 1\n",
    "        return vectorIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple term count model\n",
    "# string to vector\n",
    "def constructVector(self, wordString):\n",
    "    # 0으로 벡터 초기화\n",
    "    Vector_val = [0] * len(self.vectorKeywordIndex)\n",
    "    tokList = self.parser.tokenize(tokString)\n",
    "    tokList = self.parser.eliminatestopwords(tokList)\n",
    "    for word in toklist:\n",
    "        vector[self.vectorKeywordIndex[word]] += 1;\n",
    "        return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine Similarity\n",
    "# cosine = (X * Y) / ||X|| x ||Y||\n",
    "def cosine(vec1, vec2):\n",
    "    return float(dot(vec1,vec2) / (normj(vec1) * norm(vec2)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 키워드와 벡터 공간의 매핑을 수행 - 검색할 항목을 나타내는 임시 텍스트를 구성한 다음 코사인 측정을 통해 문서 벡터와 비교한다.\n",
    "\n",
    "def searching(self, searchinglist):\n",
    "    askVector = self.buildQueryVector(searchinglist)\n",
    "    \n",
    "    ratings = [utils.cosine(askVector, textVector) for textVector in self.documentVectors]\n",
    "    ratings.sort(reverse=True)\n",
    "    return ratings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 소스 텍스트에서 언어를 탐지하는 데 사용\n",
    "def _calculate_languages_ratios(text):\n",
    "    # {'german':2, 'french':4, 'english':1}\n",
    "    languages_ratios = {}\n",
    "    \n",
    "    tok = nltk.wordpunct_tokenize(text)\n",
    "    wor = [word.lower() for word in tok]\n",
    "    \n",
    "    # 텍스트에서 고유 불용어의 발생을 계산\n",
    "    for language in stopwords.fileids():\n",
    "        stopwords_set = set(stopwords.words(language))\n",
    "        words_set = set(words)\n",
    "        common_elements = words_set.intersection(stopwords_set)\n",
    "        languages_ratios[language] = len(common_elements)\n",
    "        # 언어 \"점수\"\n",
    "        return languages_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language(text):\n",
    "    ratios = _calculate_languages_ratios(text)\n",
    "    most_rate_language = max(ratios, key = ratios.get)\n",
    "    return most_rate_language\n",
    "\n",
    "if __name__=='__main__':\n",
    "    text = '''\n",
    "    WASHINGTON President Trump declared on Tuesday that he was withdrawing from the Iran nuclear deal, unraveling the signature foreign policy achievement of his predecessor Barack Obama, isolating the United States from its Western allies and sowing uncertainty before a risky nuclear negotiation with North Korea.The decision, while long anticipated and widely telegraphed, leaves the 2015 agreement reached by seven countries after more than two years of grueling negotiations in tatters. The United States will now reimpose the stringent sanctions it imposed on Iran before the deal and is considering new penalties.\n",
    "    '''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-7472c87e23b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlanguage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdetect_language\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-32-71bf84899248>\u001b[0m in \u001b[0;36mdetect_language\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdetect_language\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mratios\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_calculate_languages_ratios\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mmost_rate_language\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mratios\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mratios\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmost_rate_language\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-35-1aa4d310ca2a>\u001b[0m in \u001b[0;36m_calculate_languages_ratios\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mlanguage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mstopwords_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mwords_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mcommon_elements\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwords_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopwords_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mlanguages_ratios\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommon_elements\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'words' is not defined"
     ]
    }
   ],
   "source": [
    "language = detect_language(text)\n",
    "print(language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
