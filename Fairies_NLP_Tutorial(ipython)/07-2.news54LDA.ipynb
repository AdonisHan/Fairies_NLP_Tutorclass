{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import numpy as np\n",
    "import mglearn\n",
    "import konlpy\n",
    "import pandas as pd\n",
    "from konlpy.tag import *\n",
    "import matplotlib as plt\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "hannanum = Hannanum()\n",
    "kkma = Kkma()\n",
    "twitter = Twitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"C:/Users/Adonishan/nlp/data/news.txt\", \"rb\") as input:\n",
    "    data = input.read()\n",
    "    \n",
    "data = data.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "\n",
    "with open('C:/Users/Adonishan/nlp/data/news.txt', 'r') as in_file:\n",
    "    stripped = (line.strip() for line in in_file)\n",
    "    lines = (line.split(\",\") for line in stripped if line)\n",
    "    with open('news.csv', 'w') as out_file:\n",
    "        writer = csv.writer(out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 조매니저님\n",
    "\n",
    "with open('C:/Users/Adonishan/nlp/data/news.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file.readlines():\n",
    "        line_split = line.split('\\t')\n",
    "        text = line_split[0]\n",
    "        label = line_split[1].split('\\n')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'더나드리가 남성을 위한 기초화장품 ‘베르당 옴므’를 출시한다고 12일 밝혔다. 주름개선과 미백의 두 가지 효과를 한번에 얻을 수 있는 이중 기능성 화장품이다. 남성 피부는 여성 피부보다 수분 함량이 적고 두꺼워 세월이 흐르면서 노화와 술, 담배 등으로 주름이 깊어지고 얼굴색이 어두워지는 경향이 나타난다고 회사 측은 설명했다. 베르당 옴므는 ‘리벨런싱 토닉’과 ‘듀얼 모션 에센셜라이즈’ 2종으로 구성됐다. 영양보습, 수분 유지, 주름 개선 및 미백의 3단계 관리로 남성 피부에 잃어버린 빛을 되찾아 준다. 리벨런싱 토닉은 해독작용을 하는 ‘마치현 추출물’과 오염물질을 제거해주는 ‘모링가 추출물’을 함유한다. 듀얼 모션에센셜라이에는 식약청 인증을 받은 아데노신과 알부틴 성분이 들어있다. 2종이 모두 포함된 세트의 가격은 4만8000원이다.'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = kkma.nouns(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mecab = Mecab()\n",
    "# # mecab.pos(corpus[:43])\n",
    "# # data = text_cleaning(corpus)\n",
    "# # 명사만 뽑기.\n",
    "\n",
    "# data = mecab.nouns(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 명사추출 by kkma\n",
    "data = kkma.nouns(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "java.lang.VirtualMachineErrorPyRaisable",
     "evalue": "java.lang.OutOfMemoryError: GC overhead limit exceeded",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mjava.lang.VirtualMachineErrorPyRaisable\u001b[0m   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-c6b4c735e924>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 명사추출 by hannanum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhannanum\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnouns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\konlpy\\tag\\_hannanum.py\u001b[0m in \u001b[0;36mnouns\u001b[1;34m(self, phrase)\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;34m\"\"\"Noun extractor.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mtagged\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtagged\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'N'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\konlpy\\tag\\_hannanum.py\u001b[0m in \u001b[0;36mpos\u001b[1;34m(self, phrase, ntags, flatten)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mntags\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjhi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplePos09\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mntags\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m22\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjhi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplePos22\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mjava.lang.VirtualMachineErrorPyRaisable\u001b[0m: java.lang.OutOfMemoryError: GC overhead limit exceeded"
     ]
    }
   ],
   "source": [
    "# # 명사추출 by hannanum\n",
    "# data = hannanum.nouns(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "java.lang.VirtualMachineErrorPyRaisable",
     "evalue": "java.lang.OutOfMemoryError: Java heap space",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mjava.lang.VirtualMachineErrorPyRaisable\u001b[0m   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-7aa60efeff48>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 명사추출 by twitter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtwitter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnouns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\konlpy\\tag\\_twitter.py\u001b[0m in \u001b[0;36mnouns\u001b[1;34m(self, phrase)\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[1;34m\"\"\"Noun extractor.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[0mtagged\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtagged\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'Noun'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\konlpy\\tag\\_twitter.py\u001b[0m in \u001b[0;36mpos\u001b[1;34m(self, phrase, norm, stem)\u001b[0m\n\u001b[0;32m     49\u001b[0m                     \u001b[0mphrase\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m                     \u001b[0mjpype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBoolean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m                     jpype.java.lang.Boolean(stem)).toArray()\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrsplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mjava.lang.VirtualMachineErrorPyRaisable\u001b[0m: java.lang.OutOfMemoryError: Java heap space"
     ]
    }
   ],
   "source": [
    "# # 명사추출 by twitter\n",
    "# data = twitter.nouns(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tf- idf Vectorizer\n",
    "vectorizer = TfidfVectorizer(lowercase=False)\n",
    "x_list = vectorizer.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Vectorizer\n",
    "vectorizer = CountVectorizer(max_features=10000, max_df=.15)\n",
    "x_list = vectorizer.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adonishan\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# LDA\n",
    "keywords = []\n",
    "lda = LatentDirichletAllocation(n_topics=2, learning_method='batch', max_iter=25, random_state=0).fit(x_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nkeywords = []\\nlda = LatentDirichletAllocation(n_topics=10, learning_method='batch', max_iter=25, random_state=0).fit(x_list)\\n\\nfeature_names = vectorizer.get_feature_names()\\nkeywords.append(top_words(lda, feature_names, 5))\\nprint(keywords)\\n\\n\""
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "keywords = []\n",
    "lda = LatentDirichletAllocation(n_topics=10, learning_method='batch', max_iter=25, random_state=0).fit(x_list)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "keywords.append(top_words(lda, feature_names, 5))\n",
    "print(keywords)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adonishan\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lda.components_.shape: (2, 66)\n"
     ]
    }
   ],
   "source": [
    "document_topics = lda.fit_transform(x_list)\n",
    "print(\"lda.components_.shape: {}\".format(lda.components_.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting = np.argsort(lda.components_, axis=1)[:, ::-1]\n",
    "feature_names = np.array(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['12', '12일', '2종', '3단계', '4만8000원', '8000', '가격', '가지', '개선',\n",
       "       '경향', '관리', '구성', '기능성', '기초', '기초화장품', '나드리', '남성', '노화', '단계',\n",
       "       '담배', '듀얼', '리벨런싱', '링가', '마치현', '모션', '모션에센셜라이에', '물질', '미백',\n",
       "       '베르', '보습', '부틴', '설명', '성분', '세월', '세트', '셜라이즈', '수분', '아데노신',\n",
       "       '얼굴색', '여성', '영양', '영양보습', '오염', '오염물질', '옴므', '유지', '이중', '이즈',\n",
       "       '인증', '작용', '제거', '주름', '주름개선', '추출물', '출시', '토닉', '포함', '피부',\n",
       "       '한번', '함량', '함유', '해독', '해독작용', '화장품', '회사', '효과'], dtype='<U8')"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-114-1f1890620e86>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#https://www.kaggle.com/arthurtok/spooky-nlp-and-topic-modelling-tutorial/code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# https://www.kaggle.com/meiyizi/spooky-nlp-and-topic-modelling-tutorial/code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m13\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m wc = WordCloud(background_color=\"black\", max_words=10000, \n\u001b[0;32m      5\u001b[0m                mask=hcmask3, max_font_size= 40)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "#https://www.kaggle.com/arthurtok/spooky-nlp-and-topic-modelling-tutorial/code\n",
    "# https://www.kaggle.com/meiyizi/spooky-nlp-and-topic-modelling-tutorial/code\n",
    "plt.figure(figsize=(16,13))\n",
    "wc = WordCloud(background_color=\"black\", max_words=10000, \n",
    "               mask=hcmask3, max_font_size= 40)\n",
    "wc.generate(\" \".join(feature_names))\n",
    "plt.title(\"LDA\", fontsize=20)\n",
    "# plt.imshow(wc.recolor( colormap= 'Pastel1_r' , random_state=17), alpha=0.98)\n",
    "plt.imshow(wc.recolor( colormap= 'Pastel2' , random_state=17), alpha=0.98)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordCloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-111-8720acc22b56>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Generating the wordcloud with the values under the category dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m cloud = WordCloud(stopwords=STOPWORDS,\n\u001b[0m\u001b[0;32m      3\u001b[0m                           \u001b[0mbackground_color\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'black'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                           \u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2500\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                           height=1800).generate(\" \".join(feature_names))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'WordCloud' is not defined"
     ]
    }
   ],
   "source": [
    "# Generating the wordcloud with the values under the category dataframe\n",
    "cloud = WordCloud(stopwords=STOPWORDS,\n",
    "                          background_color='black',\n",
    "                          width=2500,\n",
    "                          height=1800).generate(\" \".join(feature_names))\n",
    "plt.imshow(cloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0       topic 1       \n",
      "--------      --------      \n",
      "토닉            옴므            \n",
      "모션            작용            \n",
      "구성            가지            \n",
      "미백            나드리           \n",
      "영양            포함            \n",
      "함유            경향            \n",
      "리벨런싱          함량            \n",
      "부틴            단계            \n",
      "주름            물질            \n",
      "이중            오염            \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mglearn.tools.print_topics(topics=range(2), feature_names=feature_names,\n",
    "                           sorting=sorting, topics_per_chunk=5, n_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
